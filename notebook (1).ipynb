{"cells":[{"source":"![my image caption](1.jpg)","metadata":{},"id":"391d595f-9da1-420b-bf8c-5f6ae758f889","cell_type":"markdown"},{"source":"# Description of the problem","metadata":{},"id":"1d2aa8fc-50cd-4265-bd89-e3805c3362e4","cell_type":"markdown"},{"source":"The company found out that the previous segmentation of the consumer market, grouped by geography, is not suitable for improving their marketing service. It does not display the actual picture, since there may be 1 doctor with more than 100 purchases in the region, or 7 doctors with 4 purchases. Therefore, the region is not a good predictor of the number of purchases that a doctor will make or his support needs. The company turned to us for help to build a new segmentation of doctors based on the company's data in order to improve marketing, customer service and product planning.\n\nAll the technical side will be described consistently in the course of the work. Now let's talk about the result and what characteristics distinguish the new segments.","metadata":{},"id":"39ea9d0e-5959-4303-ae8f-eebe8b7d0107","cell_type":"markdown"},{"source":"# The result of the work\nIn the course of the work, we will confirm the assumption that the region is not suitable for predicting the number of purchases, several features will also be revealed:\t\n - eneral practitioners have practically no complaints. Complaints appear in the category of doctors of the \"specialist\" type, and with the growth of their internal rating, the number of complaints also increases, more than 80% of all complaints come from \"higher\" ranks (Ambassador, Titanium, Platinum Plus, Platinum).\n - doctors of the rank of \"Ambassador\" account for more than a third of these complaints, about half of all purchases, despite the fact that there are slightly more than a quarter of such doctors in the total number.\n\nAn assumption was made about a new classification of doctors according to their internal ranking system, which will be one of the possible options for a new market segmentation.\n\nDeveloping this assumption, we combined several data frames into one and added a new categorical column \"number of purchases\", which divided our data into 3 values ('Small', 'Medium' and 'Large') depending on their number of purchases during the year.\n\nIn the resulting data, we conducted clustering with 3 main classes of doctors. According to the data, it is well read that 3 classes characterize our assumption of a new segmentation. Having grouped all doctors in this way, we have created two classification models.\nThe first predicts which group a particular doctor will fall into, without seeing data about his purchases, relying mainly on the \"Incident rate\", \"R rate\",\"Satisfaction\" and \"Experience\" of the doctor.\nThe second model tries to predict what level (in the company's internal ranking system) a doctor can get this. The model also does not see the number of purchases he has made, but is based on almost the same functions as the first model.\n\nBased on all of the above and based on the research conducted in the work, the marketing department can try to build a new strategy in the relationship of working with clients and it would be wonderful to check or refute this theory together. What is great about data analysis in this situation is that it allows you to react as quickly as possible to any changes or test new theories.","metadata":{},"id":"8bfbde6a-4a01-4361-8929-f6cc17c6d276","cell_type":"markdown"},{"source":"# Beginning of research work\n## ðŸ’¾ The data\n\nThe company stores the information you need in the following four tables. Some of the fields are anonymized to comply with privacy regulations.\n\n#### Doctors contains information on doctors. Each row represents one doctor.\n- \"DoctorID\" - is a unique identifier for each doctor.\n- \"Region\" - the current geographical region of the doctor.\n- \"Category\" - the type of doctor, either 'Specialist' or 'General Practitioner.'\n- \"Rank\" - is an internal ranking system. It is an ordered variable: The highest level is Ambassadors, followed by Titanium Plus, Titanium, Platinum Plus, Platinum, Gold Plus, Gold, Silver Plus, and the lowest level is Silver.\n- \"Incidence rate\"  and \"R rate\" - relate to the amount of re-work each doctor generates.\n- \"Satisfaction\" - measures doctors' satisfaction with the company.\n- \"Experience\" - relates to the doctor's experience with the company.\n- \"Purchases\" - purchases over the last year.\n\n#### Orders contains details on orders. Each row represents one order; a doctor can place multiple orders.\n- \"DoctorID\" - doctor id (matches the other tables).\n- \"OrderID\" - order identifier.\n- \"OrderNum\" - order number.\n- \"Conditions A through J\" - map the different settings of the devices in each order. Each order goes to an individual patient.\n\n#### Complaints collects information on doctor complaints.\n- \"DoctorID\" - doctor id (matches the other tables).\n- \"Complaint Type\" - the company's classification of the complaints.\n- \"Qty\" - number of complaints per complaint type per doctor.\n\n#### Instructions has information on whether the doctor includes special instructions on their orders.\n- \"DoctorID\" - doctor id (matches the other tables).\n- \"Instructions\" - 'Yes' when the doctor includes special instructions, 'No' when they do not.","metadata":{},"id":"2f5929b9-9d9a-457a-99ab-be05f27176f9","cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix","metadata":{},"id":"4c124434-006d-4b9d-9451-82b1e1e169db","cell_type":"code","execution_count":1,"outputs":[]},{"source":"doctors = pd.read_csv('data/doctors.csv')\ndoctors.columns = doctors.columns.str.replace(' ', '_')\n\ndoctors.head(3)","metadata":{},"id":"07ee76ec-6cc0-4cc7-b6ed-eba4feb6ca0b","cell_type":"code","execution_count":2,"outputs":[]},{"source":"orders = pd.read_csv('data/orders.csv')\norders.columns = orders.columns.str.replace(' ', '_')\n\norders.head(3)","metadata":{},"id":"2b75957b-35e3-492e-b1ab-41780128ed18","cell_type":"code","execution_count":3,"outputs":[]},{"source":"complaints = pd.read_csv('data/complaints.csv')\ncomplaints.columns = complaints.columns.str.replace(' ', '_')\n\ncomplaints.head(3)","metadata":{},"id":"fd0b2654-cc60-48de-bb1e-f97928c72e75","cell_type":"code","execution_count":4,"outputs":[]},{"source":"instructions = pd.read_csv('data/instructions.csv')\ninstructions.head(3)","metadata":{},"id":"873723e7-b56a-4663-a622-8f077ce3ae37","cell_type":"code","execution_count":5,"outputs":[]},{"source":"## The beginning of the analysis\nLet's start with the standard actions when analyzing a dataframe. Let's look at the data types, whether there are missing values and duplicates.","metadata":{},"id":"29a5fbcf-2c2a-46c5-83db-220f2549e1af","cell_type":"markdown"},{"source":"doctors.info()","metadata":{},"id":"c0ced8f6-d6b0-4561-8a9d-62be685dfaa0","cell_type":"code","execution_count":6,"outputs":[]},{"source":"doctors[doctors.duplicated()]","metadata":{},"id":"923d071f-3a71-438a-b293-dc4bd0c9ee7d","cell_type":"code","execution_count":7,"outputs":[]},{"source":"doctors[doctors.Rank.isna()]","metadata":{},"id":"15f5e3e4-5bf4-4d46-b2ac-5dccd9c9d3d3","cell_type":"code","execution_count":8,"outputs":[]},{"source":"As you can see, there is a value in the \"Satisfaction\" column that cannot be interpreted in the future, replace it with 0. Next, let's look at our data and see how many unique values there are.","metadata":{},"id":"a37bb311-414b-4916-87db-5a88ef12280b","cell_type":"markdown"},{"source":"doctors.Satisfaction = doctors.Satisfaction.replace('--','0').astype('float')","metadata":{},"id":"b8e90cac-3a44-4e79-85cf-9f31379eb96d","cell_type":"code","execution_count":9,"outputs":[]},{"source":"variables = pd.DataFrame(columns=['Variable','Number of unique values','Values'])\n\nfor i, var in enumerate(doctors.columns):\n    variables.loc[i] = [var, doctors[var].nunique(), doctors[var].unique().tolist()]\nvariables.set_index('Variable', inplace=True)    \nvariables","metadata":{},"id":"afdd18f2-638c-46d2-a386-089b67dc7b4d","cell_type":"code","execution_count":10,"outputs":[]},{"source":"## How many doctors...\nLet's answer the first question: How many doctors are there in each region?","metadata":{},"id":"d4d2f3c9-28c3-49e1-967e-bdc5b40e8f88","cell_type":"markdown"},{"source":"count_doct_per_region = doctors.groupby('Region').count()\\\n\t\t\t\t\t\t\t   .rename(columns={\"DoctorID\": \"count\"})['count']\\\n\t\t\t\t\t\t\t   .sort_values(ascending =False)\\\n\t\t\t\t\t\t\t   .reset_index()\ncount_doct_per_region.head(10)","metadata":{},"id":"e2b3eb25-932b-44a4-be62-e473b5301c87","cell_type":"code","execution_count":11,"outputs":[]},{"source":"We received a table with the number of doctors in each region. \n#### The largest region with the code \"1 13\" has 34 doctors.","metadata":{},"id":"4fb76d94-900f-48e6-93a4-4fdfc2b9d4f0","cell_type":"markdown"},{"source":"Now let's see how much the average number of purchases in each region is.","metadata":{},"id":"d03335e9-e060-438a-ba09-c40ff3d18c63","cell_type":"markdown"},{"source":"mean_purch_per_region = doctors.groupby('Region')\\\n\t\t\t\t\t\t\t   .agg({'Purchases' :['mean','count','sum']})\\\n\t\t\t\t\t\t\t   .sort_values(by =[('Purchases','mean')],ascending =False)\\\n \t\t\t\t\t\t\t   .reset_index().round(1)\nmean_purch_per_region.head(10)","metadata":{},"id":"3f511dcf-eae0-40bd-aed8-eda8fad1ba15","cell_type":"code","execution_count":12,"outputs":[]},{"source":"As we saw, the grouping, as it was at the firm initially, is not the most successful, because only two regions out of the top 10 in terms of the number of doctors entered the top 10 in terms of the average number of purchases, taking 9th and 10th place there. \n#### The first places in this list are occupied by regions with only 1 doctor each.","metadata":{},"id":"4dbc0b50-375c-4733-9e7b-c8bba9826730","cell_type":"markdown"},{"source":"Having answered the first of the questions posed in the competition, then we will try to find the relationship between purchases and complaints.","metadata":{},"id":"3352d10a-537c-4100-8cef-2be22ea2691c","cell_type":"markdown"},{"source":"The dataframe itself with the number of complaints won 't give us much information . In it, we can see the distribution of complaints by their type and the number of doctors in them. Let's combine this dataframe with data about purchases from the dataframe about doctors and try to find the relationship between purchases and complaints.","metadata":{},"id":"24232e4a-c08b-43f1-9eea-0900d0d5180e","cell_type":"markdown"},{"source":"It can be seen that most of the complaints, 451 complaints, are displayed as \"correct\" and the number of doctors is 205. Also\n, the type of complaint classified as \"unknown\" is quite important.","metadata":{},"id":"9d60676a-001f-4584-b899-86e52135f5aa","cell_type":"markdown"},{"source":"complaints.groupby('Complaint_Type').agg({'Qty' :['count','sum']})","metadata":{},"id":"baf63d4c-230e-40a7-a3e5-b98c56f36cd8","cell_type":"code","execution_count":13,"outputs":[]},{"source":"After combining the data, we received 125 lines about purchases and complaints from doctors. Of these, 74 are unique values.","metadata":{},"id":"be42a083-7777-47ed-a3c3-3ddee3edcb66","cell_type":"markdown"},{"source":"doct_compl = pd.merge(doctors,complaints, how ='inner',on ='DoctorID')","metadata":{},"id":"d3834049-a033-442f-9faa-02c98276de46","cell_type":"code","execution_count":14,"outputs":[]},{"source":"categ_rank = doct_compl.groupby(['Category','Rank']).agg({'Qty' :['count','sum'], 'Purchases' :['sum']})\npie, ax = plt.subplots(figsize=[12,7])\nplt.pie(x=categ_rank[('Qty', 'count')], autopct=\"%.f%%\",labels =categ_rank.index);\nplt.title(\"The number of complaints depending on the specialization and rank of the doctor\", fontsize=17);\n\n","metadata":{},"id":"5a99a288-731e-456a-9bac-bb901454125d","cell_type":"code","execution_count":15,"outputs":[]},{"source":"The diagram shows that general practitioners have practically no complaints. Complaints appear in the category of doctors of the \"specialist\" type, and with the growth of their internal ranking, the number of complaints also increases. It is clearly visible that more than 80% of complaints come from \"higher\" ranks (Ambassador,Titanium, Platinum Plus, Platinum). Let's move on to further consideration of this subgroup.","metadata":{},"id":"5714e228-5644-484f-aee2-80846feb8f33","cell_type":"markdown"},{"source":"purch_compl = doct_compl.query('Category == \"Specialist\"')\\\n\t\t\t\t\t\t.groupby(['Complaint_Type','Rank'])\\\n\t\t\t\t\t\t.agg({'Qty' :['count','sum'], 'Purchases' :['sum']})\\\n\t\t\t\t\t\t.sort_values(by = ('Qty', 'count'), ascending = False)\npurch_compl[purch_compl[('Qty', 'count')] >2]","metadata":{},"id":"643bfffe-41da-45ac-ac4f-13cd3048c0c5","cell_type":"code","execution_count":16,"outputs":[]},{"source":"## Relationship between purchases and complaints\nFrom the available dataframe about doctors, it is difficult to find an explanation for why there are so many dissatisfied among highly qualified doctors. Neither the breakdown into regions, nor the level of \"satisfaction\" and \"experience\" provide an explanation for this effect.We found out that the number of purchases is closely related to the internal ranking of the doctor.\n\nAccording to calculations, it turns out that doctors of the highest rank of \"Ambassador\" account for 37% of complaints (based on the combined dataframe) , 49% of all purchases, despite the fact that such doctors are slightly more than a quarter of the total number. So it is obvious that our assumptions about the new classification of doctors according to their internal ranking system is one of the possible options for a new market segmentation. Let's move on to this question in more detail and try to answer it.","metadata":{},"id":"dae04266-5595-4212-b4e5-0716b9064036","cell_type":"markdown"},{"source":"## The most popular orders\nHaving studied the dataframe \"Orders\", we can say that orders prevail in which all conditions from \"A\" to \"I\" are False, such orders are 101 of the total number, for the next number where \"Condition H\" is True, the rest are False of such 23 orders. 14 orders where \"G\" and \"H\" are true, and 10 where only \"Condition A\" is true.\n\nLet's see which doctors make the most popular orders.","metadata":{},"id":"21fc919f-f665-46b1-ad89-2cf91cd6129a","cell_type":"markdown"},{"source":"merge = pd.merge(doct_compl,orders, how ='inner',on ='DoctorID')\n\nmerge['Condition_C'] = merge['Condition_C'].fillna(merge['Condition_C'].mode()[0])\nmerge['Condition_F'] = merge['Condition_F'].fillna(merge['Condition_F'].mode()[0])\nmerge['Condition_G'] = merge['Condition_G'].fillna(merge['Condition_G'].mode()[0])\nmerge['Condition_J'] = merge['Condition_J'].fillna(merge['Condition_J'].mode()[0])\n","metadata":{},"id":"50e94669-98ea-4500-a790-ffa13382a88e","cell_type":"code","execution_count":17,"outputs":[]},{"source":"variables = pd.DataFrame(columns=['Variable','Number of unique values','Values'])\n\nfor i, var in enumerate(merge.columns):\n    variables.loc[i] = [var, merge[var].nunique(), merge[var].unique().tolist()]\nvariables.set_index('Variable', inplace=True)    \nvariables","metadata":{},"id":"e4cf0154-1fea-45a0-bb5f-69150f9fed21","cell_type":"code","execution_count":18,"outputs":[]},{"source":"most_pop_order = merge.query('Condition_A ==False and Condition_B ==False and Condition_C ==False and Condition_D ==False and Condition_F ==False and Condition_G ==False and Condition_H ==False and Condition_I ==False')","metadata":{},"id":"f8b91c8f-b265-4ff4-9578-c1b4e00bd19e","cell_type":"code","execution_count":19,"outputs":[]},{"source":"most_pop_order.head(3)","metadata":{},"id":"527e2eee-eb9e-4c96-a7b0-ce7646d23640","cell_type":"code","execution_count":20,"outputs":[]},{"source":"most_pop_order.groupby('Rank').agg({'Purchases' :['count','sum']})\\\n\t\t\t\t\t\t.sort_values(by = ('Purchases', 'count'), ascending = False)","metadata":{},"id":"54b411a0-bff5-415c-8f2b-921b54acb479","cell_type":"code","execution_count":21,"outputs":[]},{"source":"And again, we received that doctors with the rank of \"Ambassador\" issued several times more of the most popular orders than all the others.\n\n## Let's start clustering our sample.","metadata":{},"id":"013cdce9-389a-417e-9e2b-a89524b629a2","cell_type":"markdown"},{"source":"We will first create a new categorical column in the date frame, which will summarize the number of purchases by three criteria:\n\n1 \"Small\" - less than 15 purchases\n\n2 \"Medium\" from 15 to 30 purchases\n\n3 \"Large \" more than 30 purchases\n\nNext, we will use the KMeans algorithm and cluster our data. We have determined that our optimal data is divided into 3 clusters.","metadata":{},"id":"5bf63ccf-c121-4a18-b08a-bd1d8f85365e","cell_type":"markdown"},{"source":"def to_category(item):\n    if item['Purchases'] <15:\n        return 'Small'\n    elif item['Purchases'] >=15 and item['Purchases'] <30:\n    \treturn 'Medium'\n    elif item['Purchases'] >=30:\n    \treturn 'Large'\n  ","metadata":{},"id":"5c2a7186-03df-4a15-a936-34777fa19b4b","cell_type":"code","execution_count":22,"outputs":[]},{"source":"most_pop_order[\"Number_orders\"] = most_pop_order.apply(to_category, axis=1)\n\nsamples = most_pop_order[[ 'Incidence_rate', 'R_rate',\n       'Satisfaction', 'Experience', 'Purchases', 'Qty',\n        'Condition_A', 'Condition_B', 'Condition_C',\n       'Condition_D', 'Condition_F', 'Condition_G', 'Condition_H',\n       'Condition_I' ]]","metadata":{},"id":"1bbf957a-e34a-46f7-84cb-e97f3de71ab1","cell_type":"code","execution_count":23,"outputs":[]},{"source":"model = KMeans(n_clusters = 3,random_state=1)\nlabels = model.fit_predict(samples)\n\ndf = pd.DataFrame({'labels': labels, 'Number of purchases': most_pop_order[\"Number_orders\"]})\nct = pd.crosstab(df['labels'],df['Number of purchases'])\n\nprint(ct)\n","metadata":{},"id":"f612c5ac-21a1-468b-a22a-0fb5a6da4018","cell_type":"code","execution_count":24,"outputs":[]},{"source":"The cross-table shows that doctors are really well divided into 3 groups, depending on the purchases they have made.","metadata":{},"id":"bb4b5e7d-1ec2-44bc-8f81-e3af41a16b6d","cell_type":"markdown"},{"source":"tsne = TSNE(learning_rate=50,random_state=1)\ntsne_features = tsne.fit_transform(samples)\n\nxs = tsne_features[:,0]\n\n\nys = tsne_features[:,1]\n\nplt.figure(figsize = [10,5])\nplt.title(\"Visualization for our clustering\", fontsize = 20)\nplt.xlabel(\"Dimension 1\", fontsize = 15)\nplt.ylabel(\"Dimension 2\", fontsize = 15)\nplt.scatter(xs, ys, c=labels)\nplt.show()","metadata":{},"id":"d9d537e7-6a64-4996-87cf-7c9115d8bcde","cell_type":"code","execution_count":25,"outputs":[]},{"source":"The graph also perfectly shows that we have all doctors can be divided into three groups, they certainly have some data outliers from these groups. Attempts to group them again by a different number of purchases, or to increase the number of clusters, did not bring visible results. So it was decided to leave the division into 3 groups and see if the model works well on new data, which contains all types of orders and not just the most popular.","metadata":{},"id":"6a4600ad-5d34-42a9-a9c3-849aefdf60c3","cell_type":"markdown"},{"source":"merge[\"Number_orders\"] = merge.apply(to_category, axis=1)\nmerge[['Rank','Number_orders']] = merge[['Rank','Number_orders']].astype('category')\n\nsamples1 = merge[[ 'Incidence_rate', 'R_rate',\n       'Satisfaction', 'Experience', 'Purchases', 'Qty',\n        'Condition_A', 'Condition_B', 'Condition_C',\n       'Condition_D', 'Condition_F', 'Condition_G', 'Condition_H',\n       'Condition_I', ]]\nlabels1 = model.predict(samples1)\ndf_all = pd.DataFrame({'labels': labels1, 'Number of purchases': merge[\"Number_orders\"]})\nct_all = pd.crosstab(df_all['labels'],df_all['Number of purchases'])\n\nprint(ct_all)","metadata":{},"id":"278116dc-8f31-4b6f-ba37-4bcf6ef0e388","cell_type":"code","execution_count":26,"outputs":[]},{"source":"It can be seen that the combined dataframe is also clearly divided into 3 main clusters.","metadata":{},"id":"a291c528-db4a-4cb6-bef9-ddb2644fbf4a","cell_type":"markdown"},{"source":"### A few words about the functions\nIn the final report, I did not include my calculations about reducing the number of functions used in the model (PCA), but I still need to say a few words. Since the data is small enough, no special computing power is required for the calculation. Therefore, to use all the functions in creating a model, to try to reduce their number, as a result, our analysis will not receive any visible advantages. I want to say that the data from the \"Orders\" table practically does not affect the decision of the models, in the main function that have an impact is information about the qualifications of the doctor, satisfaction with the company (about which I would also like to say a few words below), Incident rate\" and \"R rate\".\n\nUnfortunately, the data in the \"Satisfaction\" column remained not fully understood by me, and their meager description contributed to this. Because it is not entirely clear on what scale this value is measured, from 0 to 100, and then negative values can be considered an error, or negative values are an outgrowth of the negative to the company, then what attitude does the doctor express to the company if the value in this column is 0. Of course, the presence of \"feedback\" from the company would contribute to a better understanding, and in consequence and data analysis.\n#### Therefore, this column in the data leaves more questions than answers them.","metadata":{},"id":"7e1a4b4e-9d69-4da2-80c9-d0811c5bd415","cell_type":"markdown"},{"source":"### Let's move on to building machine learning models.\n\nThe first model will predict which group the doctor will fall into, using information about his experience, satisfaction with the company, the number of complaints, data that relate to his repeated work and information about the order itself. We have specifically deleted all the information related to the purchase of orthopedic products by a doctor so that the model could predict the newly joined doctors.","metadata":{},"id":"a76fcc24-372b-407f-a2ed-d7bd51a9bf73","cell_type":"markdown"},{"source":"y = merge.loc[:,\"Number_orders\"]\nX =merge.loc[:,['Incidence_rate', 'R_rate','Satisfaction',\n        'Experience', 'Qty',\n        'Condition_A', 'Condition_B', 'Condition_C',\n       'Condition_D', 'Condition_F', 'Condition_G', 'Condition_H',\n       'Condition_I', ]]","metadata":{},"id":"84b94c08-5b8b-4383-958e-954cd33463f2","cell_type":"code","execution_count":27,"outputs":[]},{"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\nrandom_state=1)\nX_test, X_val, y_test, y_val = train_test_split(X_test,y_test , test_size=0.4,\nrandom_state=1)\n\nlr_1 = LogisticRegression(C=17.171,)\nsvc_1 = SVC(kernel = 'linear' , C = 1,gamma = 1e-05,  )\nforest = RandomForestClassifier(n_estimators=50, max_depth=9,min_samples_leaf=1, min_samples_split=2 ,random_state=1)\n\n\nsvc_1.fit(X_train, y_train)\nforest.fit(X_train, y_train)\nlr_1.fit(X_train,y_train)\n\nsvc_pred1 = svc_1.predict(X_test)\nforest_pred = forest.predict(X_test)\nlr_pred1 = lr_1.predict(X_test)\n\nprint(\"Accuracy of logistic regression classifier: \", accuracy_score(y_test, lr_pred1))\nprint(\"Accuracy of the svc model = %f\" %accuracy_score(y_test, svc_pred1) )\nprint(\"Accuracy of the forest model = %f\" %accuracy_score(y_test, forest_pred) )","metadata":{},"id":"7d24423c-6c64-4ff9-847a-892ff6ed020d","cell_type":"code","execution_count":28,"outputs":[]},{"source":"print(\"f1-score of the svc model on test data = %f\" %f1_score(y_test, svc_pred1, average='weighted') )\nprint(\"f1-score of the forest model on test data = %f\" %f1_score(y_test, forest_pred, average='weighted'))\nprint(\"f1-score of the logistic regression on test data = %f\" %f1_score(y_test, lr_pred1, average='weighted'))","metadata":{},"id":"a320d28f-3acf-4e9e-8105-702a5fb3eff8","cell_type":"code","execution_count":29,"outputs":[]},{"source":"print(\"f1-score of the svc model on validation data = %f\" %f1_score(y_val, svc_1.predict(X_val), average='weighted') )\nprint(\"f1-score of the tree model on validation data = %f\" %f1_score(y_val, forest.predict(X_val), average='weighted') )\nprint(\"f1-score of the  logistic regression on validation data = %f\" %f1_score(y_val, lr_1.predict(X_val), average='weighted') )\n","metadata":{},"id":"88ea916a-7cf7-4110-baf0-bbf77d557bde","cell_type":"code","execution_count":30,"outputs":[]},{"source":"print(\"Accuracy of the forest model based on verification data = {0:.2f}\".format(accuracy_score(y_val, forest.predict(X_val))))\nprint(\"Accuracy of the logistic regression classifier based on verification data = {0:.2f}\".format(accuracy_score(y_val, lr_1.predict(X_val))))\nprint(\"Accuracy of the svc model based on verification data = {0:.2f}\".format(accuracy_score(y_val, svc_1.predict(X_val))))","metadata":{},"id":"c8e04632-e204-4c41-b701-c80d6307103c","cell_type":"code","execution_count":31,"outputs":[]},{"source":"importances = pd.Series(data=forest.feature_importances_,\n                        index= X_train.columns)\n\nimportances_sorted = importances.sort_values()\n\nplt.figure(figsize = [10,5])\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.xlabel(\"Degree of importance of functions\", fontsize = 15)\nplt.ylabel(\"Model functions\", fontsize = 15);\nplt.title('Features Importances', fontsize = 15)\nplt.show()","metadata":{},"id":"a5a2b5d2-a2a7-4de5-8e41-cbca8924c3f9","cell_type":"code","execution_count":32,"outputs":[]},{"source":"#### By building our first model and adjusting its parameters, we managed to achieve 94% accuracy on the test data and very low error.\nThe graph above shows which functions are most important for building a model.","metadata":{},"id":"b31cc444-14df-4547-9c35-633eb4440dbc","cell_type":"markdown"},{"source":"### Let's move on to building our second model...\nwhich will try to predict what internal rank a doctor will receive in our company.\nFirst, let's combine all the data that we have into one dataframe.","metadata":{},"id":"963941b3-d27e-4232-bbd9-0c301851b65c","cell_type":"markdown"},{"source":"\ndf1 = pd.merge(doctors,orders, how ='left',on ='DoctorID')\ndf2 = pd.merge(df1,complaints, how ='left',on ='DoctorID')\ndf = pd.merge(df2,instructions, how ='inner',on ='DoctorID')","metadata":{},"id":"719a25f2-c61e-4782-92f4-a67a6d53d147","cell_type":"code","execution_count":33,"outputs":[]},{"source":"df['Condition_C'] = df['Condition_C'].fillna(df['Condition_C'].mode()[0])\ndf['Condition_F'] = df['Condition_F'].fillna(df['Condition_F'].mode()[0])\ndf['Condition_G'] = df['Condition_G'].fillna(df['Condition_G'].mode()[0])\ndf['Condition_J'] = df['Condition_J'].fillna(df['Condition_J'].mode()[0])\n\ndf['Condition_A'] = df['Condition_A'].fillna(df['Condition_A'].mode()[0])\ndf['Condition_B'] = df['Condition_B'].fillna(df['Condition_B'].mode()[0])\ndf['Condition_H'] = df['Condition_H'].fillna(df['Condition_H'].mode()[0])\ndf['Condition_I'] = df['Condition_I'].fillna(df['Condition_I'].mode()[0])\ndf['Condition_D'] = df['Condition_D'].fillna(df['Condition_D'].mode()[0])\ndf.fillna(0, inplace = True)\n","metadata":{},"id":"204c758a-c405-4b1c-bf20-45ab838ba0e5","cell_type":"code","execution_count":34,"outputs":[]},{"source":"one_hot = pd.get_dummies(df['Instructions']).rename(columns={'Yes':'Instructions_yes'})\ndf = df.join(one_hot)\n\ndf = df.drop(columns= ['Instructions','No'],axis = 1)\ndf.head()","metadata":{},"id":"e208bd5d-00c9-4c02-acf8-6e64dc8c05ad","cell_type":"code","execution_count":35,"outputs":[]},{"source":"y1 = df.loc[:,\"Rank\"]\nX1 =df.loc[:,['Incidence_rate', 'R_rate','Satisfaction', 'Experience','Condition_A','Condition_B', 'Condition_C', 'Condition_D','Condition_F','Condition_G', 'Condition_H', 'Condition_I','Qty','Instructions_yes']]","metadata":{},"id":"0700f715-2786-46c0-91b9-3f09fba6a8a3","cell_type":"code","execution_count":36,"outputs":[]},{"source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.35,\nrandom_state=11)\nX_test1, X_val1, y_test1, y_val1 = train_test_split(X_test1,y_test1 , test_size=0.4,\nrandom_state=11)\n\ntree = DecisionTreeClassifier(max_depth= 11, min_samples_leaf= 1, min_samples_split= 2)\nlr = LogisticRegression(C=17.171,)\nsvc = SVC(kernel = 'linear' , C = 1,gamma = 1e-05,  )\n\nsvc.fit(X_train1, y_train1)\ntree.fit(X_train1, y_train1)\nlr.fit(X_train1,y_train1)\n\nsvc_pred = svc.predict(X_test1)\ntree_pred = tree.predict(X_test1)\nlr_pred = lr.predict(X_test1)\nprint(\"Accuracy of the tree model = %f\" %accuracy_score(y_test1, tree_pred) )\nprint(\"Accuracy of logistic regression classifier: \", accuracy_score(y_test1, lr_pred))\nprint(\"Accuracy of the svc model = %f\" %accuracy_score(y_test1, svc_pred) )\n","metadata":{},"id":"01dab38d-dc7b-423a-a85d-d868af9e761d","cell_type":"code","execution_count":37,"outputs":[]},{"source":"print(\"f1-score of the svc model on test data = %f\" %f1_score(y_test1, svc_pred, average='weighted') )\nprint(\"f1-score of the tree model on test data = %f\" %f1_score(y_test1, tree_pred, average='weighted'))\nprint(\"f1-score of the logistic regression on test data = %f\" %f1_score(y_test1, lr_pred, average='weighted'))","metadata":{},"id":"d1eb9893-a793-4e9a-916e-350f2b6066ae","cell_type":"code","execution_count":38,"outputs":[]},{"source":"print(\"f1-score of the svc model on validation data = %f\" %f1_score(y_val1, svc.predict(X_val1), average='weighted') )\nprint(\"f1-score of the tree model on validation data = %f\" %f1_score(y_val1, tree.predict(X_val1), average='weighted') )\nprint(\"f1-score of the  logistic regression on validation data = %f\" %f1_score(y_val1, lr.predict(X_val1), average='weighted') )\n","metadata":{},"id":"8501c1f5-c67b-45c8-99e2-6c4bc6d9f23e","cell_type":"code","execution_count":39,"outputs":[]},{"source":"print(\"Accuracy of the tree model based on verification data = {0:.2f}\".format(accuracy_score(y_val1, tree.predict(X_val1))))\nprint(\"Accuracy of the logistic regression classifier based on verification data = {0:.2f}\".format(accuracy_score(y_val1, lr.predict(X_val1))))\nprint(\"Accuracy of the svc model based on verification data = {0:.2f}\".format(accuracy_score(y_val1, svc.predict(X_val1))))\n","metadata":{},"id":"ff134dee-6994-486f-93eb-847e9e38d23d","cell_type":"code","execution_count":40,"outputs":[]},{"source":"importances1 = pd.Series(data=tree.feature_importances_,\n                        index= X_train1.columns)\n\nimportances_sorted1 = importances1.sort_values()\n\nplt.figure(figsize = [10,5])\nimportances_sorted1.plot(kind='barh', color='lightgreen')\nplt.xlabel(\"Degree of importance of functions\", fontsize = 15)\nplt.ylabel(\"Model functions\", fontsize = 15);\nplt.title('Features Importances', fontsize = 15)\nplt.show()","metadata":{},"id":"787a7e8e-40aa-4265-887f-c8259b062b61","cell_type":"code","execution_count":41,"outputs":[]},{"source":"## Conclusion...\nAfter training and testing the models, we obtained good accuracy for the multiclass classification problem, both in the first and in the second case. The models are good at predicting which segments our doctors will fall into and what internal rank they will have. All this , of course , still requires checking for new data .\n\n### We can say that we have completed the tasks set in this competition and answered all the questions of interest.\n","metadata":{},"id":"d2e12c8c-faf9-4a4b-84eb-1b0b015955cb","cell_type":"markdown"},{"source":"Once again, the beauty of data analysis and machine learning is that if there is feedback from the company and the marketing department, receiving explanations on our data, and not independently interpreting their values, you can quickly influence our models, make changes to them and their goals.","metadata":{},"id":"80fa29e0-8f64-4fe1-9ad1-4692aa6fbcb1","cell_type":"markdown"},{"source":"## Thank you for reading to the end!","metadata":{},"id":"324ee228-093a-444b-9025-1f7b2fa494ec","cell_type":"markdown"},{"source":"","metadata":{},"id":"2082cae8-fe11-4b2e-aa2d-f772d50f3f45","cell_type":"markdown"},{"source":"## ðŸ’ª Competition challenge\n\nCreate a report that covers the following:\n1. How many doctors are there in each region? What is the average number of purchases per region?\n2. Can you find a relationship between purchases and complaints?\n3. Define new doctor segments that help the company improve marketing efforts and customer service.\n4. Identify which features impact the new segmentation strategy the most.\n5. Your team will need to explain the new segments to the rest of the company. Describe which characteristics distinguish the newly defined segments.","metadata":{},"id":"ea5ecc34-7949-4090-9129-684c491c05cb","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}